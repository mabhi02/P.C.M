{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our tiny custom modules instead of the original ones\n",
    "from utils.feature_extraction_small import TinyFeatureExtractionNetwork, TinyPhaseCorrelationTensorComputation, ImagePreprocessor\n",
    "from utils.manifold_learning import ManifoldLearningModule\n",
    "from utils.topological_small import TinyTopologicalFeatureExtraction\n",
    "from utils.classification import ClassificationNetwork\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class DimensionAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapter module to match dimensions between tiny feature extraction\n",
    "    and the original manifold learning module\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=64, output_dim=128):\n",
    "        super(DimensionAdapter, self).__init__()\n",
    "        \n",
    "        # Simple 1x1 convolution to change channel dimensions\n",
    "        self.adapter = nn.Conv2d(input_dim, output_dim, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.adapter(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_files, labels):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        \n",
    "        # Define transforms using standard torchvision transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            # Open image with PIL and convert to RGB\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                img_tensor = self.transform(img)\n",
    "                return img_tensor, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            # Return a placeholder tensor and the label\n",
    "            return torch.ones(3, 256, 256), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from train.csv file\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_path = r'C:\\Users\\mabhi\\.cache\\kagglehub\\datasets\\alessandrasala79\\ai-vs-human-generated-dataset\\versions\\4\\train.csv'\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"CSV columns: {df.columns.tolist()}\")\n",
    "print(f\"First few rows of the CSV:\\n{df.head()}\")\n",
    "\n",
    "# Get the base path for the images\n",
    "base_path = os.path.dirname(csv_path)\n",
    "\n",
    "# Parse file paths and labels from CSV\n",
    "all_files = []\n",
    "labels = []\n",
    "\n",
    "# Extract file paths and labels from the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    file_path = os.path.join(base_path, row['file_name'])\n",
    "    label = int(row['label'])  # Assuming 0 = Natural, 1 = AI\n",
    "    \n",
    "    # Verify that the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        all_files.append(file_path)\n",
    "        labels.append(label)\n",
    "    else:\n",
    "        print(f\"Warning: File not found: {file_path}\")\n",
    "\n",
    "print(f\"Total images found: {len(all_files)}\")\n",
    "print(f\"Natural images: {labels.count(0)}\")\n",
    "print(f\"AI-generated images: {labels.count(1)}\")\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CustomImageDataset(all_files, labels)\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders with smaller batch size\n",
    "batch_size = 2  # Reduced from 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Check a batch of data\n",
    "try:\n",
    "    images, labels = next(iter(train_loader))\n",
    "    print(f\"Batch shape: {images.shape}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    \n",
    "    # Visualize a few images\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(min(4, len(images))):\n",
    "        plt.subplot(1, 4, i+1)\n",
    "        img = images[i].permute(1, 2, 0).numpy()\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Label: {'Natural' if labels[i] == 0 else 'AI'}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading batch: {e}\")\n",
    "    print(\"This might indicate issues with image loading or dataset structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model components - now using tiny versions\n",
    "feature_network = TinyFeatureExtractionNetwork().to(device)\n",
    "tensor_computer = TinyPhaseCorrelationTensorComputation().to(device)\n",
    "\n",
    "# Add a dimension adapter to match the tiny tensor output (64 channels) \n",
    "# to the original manifold module input (128 channels)\n",
    "class DimensionAdapter(nn.Module):\n",
    "    def __init__(self, input_dim=64, output_dim=128):\n",
    "        super(DimensionAdapter, self).__init__()\n",
    "        self.adapter = nn.Conv2d(input_dim, output_dim, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.adapter(x)\n",
    "\n",
    "# Create the adapter\n",
    "dim_adapter = DimensionAdapter().to(device)\n",
    "\n",
    "# Initialize the manifold module\n",
    "manifold_module = ManifoldLearningModule().to(device)\n",
    "\n",
    "# For topological analysis, we need to create point clouds from manifold features\n",
    "# This is a simplified version for training\n",
    "class PointCloudGenerator(nn.Module):\n",
    "    def __init__(self, num_points=25, noise_scale=0.1):  # Reduced from 50 to 25 points\n",
    "        super().__init__()\n",
    "        self.num_points = num_points\n",
    "        self.noise_scale = noise_scale\n",
    "    \n",
    "    def forward(self, features):\n",
    "        batch_size, feature_dim = features.size()\n",
    "        \n",
    "        # Expand features to create a point cloud\n",
    "        point_cloud = features.unsqueeze(1).expand(-1, self.num_points, -1)\n",
    "        \n",
    "        # Add noise for a more interesting point cloud\n",
    "        noise = torch.randn(batch_size, self.num_points, feature_dim, device=features.device) * self.noise_scale\n",
    "        point_cloud = point_cloud + noise\n",
    "        \n",
    "        return point_cloud\n",
    "\n",
    "point_cloud_generator = PointCloudGenerator().to(device)\n",
    "\n",
    "# Use our TinyTopologicalFeatureExtraction\n",
    "topo_module = TinyTopologicalFeatureExtraction(\n",
    "    input_dim=32,           # Match the output dimension from TinyFeatureExtractionNetwork\n",
    "    hidden_dim=32,          # Reduced hidden dimension\n",
    "    output_dim=16,          # Reduced output dimension\n",
    "    max_edge_length=2.0,    # Maximum edge length for filtration\n",
    "    num_filtrations=10,     # Reduced number of filtration values\n",
    "    max_dimension=0         # Only dimension 0 for maximum efficiency\n",
    ").to(device)\n",
    "\n",
    "# Adjust classifier to work with the reduced dimensions from our tiny modules\n",
    "classifier = ClassificationNetwork(\n",
    "    manifold_dim=32,        # Match the output dimension from TinyFeatureExtractionNetwork\n",
    "    topo_dim=16,            # Match the output dimension from TinyTopologicalFeatureExtraction\n",
    "    feature_dim=32,         # Reduced feature dimension\n",
    "    hidden_dim=64,          # Reduced hidden dimension\n",
    "    dropout=0.1             # Lower dropout to compensate for fewer parameters\n",
    ").to(device)\n",
    "\n",
    "# Print model sizes\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Tiny Feature Extraction Network: {count_parameters(feature_network):,} parameters\")\n",
    "print(f\"Tiny Phase Correlation Tensor Computation: {count_parameters(tensor_computer):,} parameters\")\n",
    "print(f\"Dimension Adapter: {count_parameters(dim_adapter):,} parameters\")\n",
    "print(f\"Manifold Learning Module: {count_parameters(manifold_module):,} parameters\")\n",
    "print(f\"Tiny Topological Feature Extraction: {count_parameters(topo_module):,} parameters\")\n",
    "print(f\"Classification Network: {count_parameters(classifier):,} parameters\")\n",
    "total_params = (count_parameters(feature_network) + count_parameters(tensor_computer) + \n",
    "               count_parameters(dim_adapter) + count_parameters(manifold_module) + \n",
    "               count_parameters(topo_module) + count_parameters(classifier))\n",
    "print(f\"Total: {total_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "classification_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# VAE loss for manifold learning\n",
    "def vae_loss(x_recon, x, mu, logvar, beta=1.0):\n",
    "    # Reconstruction loss\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction='sum') / x.size(0)\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "    \n",
    "    return recon_loss + beta * kl_loss\n",
    "\n",
    "# Define optimizers\n",
    "# We'll use separate optimizers for each component\n",
    "feature_optimizer = optim.Adam(list(feature_network.parameters()) + \n",
    "                              list(tensor_computer.parameters()), lr=1e-4)\n",
    "manifold_optimizer = optim.Adam(manifold_module.parameters(), lr=1e-4)\n",
    "topo_optimizer = optim.Adam(topo_module.parameters(), lr=1e-4)\n",
    "classifier_optimizer = optim.Adam(classifier.parameters(), lr=1e-4)\n",
    "\n",
    "# Learning rate schedulers\n",
    "feature_scheduler = optim.lr_scheduler.ReduceLROnPlateau(feature_optimizer, mode='min', factor=0.5, patience=5)\n",
    "manifold_scheduler = optim.lr_scheduler.ReduceLROnPlateau(manifold_optimizer, mode='min', factor=0.5, patience=5)\n",
    "topo_scheduler = optim.lr_scheduler.ReduceLROnPlateau(topo_optimizer, mode='min', factor=0.5, patience=5)\n",
    "classifier_scheduler = optim.lr_scheduler.ReduceLROnPlateau(classifier_optimizer, mode='min', factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reduce batch size\n",
    "batch_size = 4  # Change this in your DataLoader creation\n",
    "\n",
    "# 2. Use gradient accumulation\n",
    "gradient_accumulation_steps = 2\n",
    "\n",
    "# Modify the train_epoch function to use gradient accumulation and dimension adapter\n",
    "def train_epoch_with_accumulation(train_loader, epoch, accumulation_steps=2):\n",
    "    # Set models to training mode\n",
    "    feature_network.train()\n",
    "    tensor_computer.train()\n",
    "    dim_adapter.train()  # Don't forget to train the adapter\n",
    "    manifold_module.train()\n",
    "    topo_module.train()\n",
    "    classifier.train()\n",
    "    \n",
    "    # Initialize metrics\n",
    "    total_loss = 0\n",
    "    total_feature_loss = 0\n",
    "    total_manifold_loss = 0\n",
    "    total_topo_loss = 0\n",
    "    total_classifier_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Training loop\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    \n",
    "    # Zero gradients at the start\n",
    "    feature_optimizer.zero_grad()\n",
    "    manifold_optimizer.zero_grad()\n",
    "    topo_optimizer.zero_grad()\n",
    "    classifier_optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(progress_bar):\n",
    "        # Move data to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass through feature extraction\n",
    "        features, _ = feature_network(images)\n",
    "        phase_tensor = tensor_computer(features)\n",
    "        \n",
    "        # Apply dimension adapter to match channel count\n",
    "        adapted_tensor = dim_adapter(phase_tensor)\n",
    "        \n",
    "        # Forward pass through manifold learning\n",
    "        manifold_features, (mu, logvar, z) = manifold_module(adapted_tensor)\n",
    "        \n",
    "        # Reconstruct for VAE loss\n",
    "        recon_tensor = manifold_module.decode(z)\n",
    "        manifold_loss = manifold_module.get_loss(adapted_tensor, recon_tensor, mu, logvar)[0]\n",
    "        \n",
    "        # Generate point cloud for topological analysis\n",
    "        point_cloud = point_cloud_generator(manifold_features)\n",
    "        \n",
    "        # Forward pass through topological analysis - now using TinyTopologicalFeatureExtraction\n",
    "        topo_features, _ = topo_module(point_cloud)\n",
    "        \n",
    "        # Forward pass through classifier\n",
    "        logits, probs, uncertainty = classifier(manifold_features, topo_features)\n",
    "        \n",
    "        # Compute losses\n",
    "        classifier_loss = classification_criterion(logits, labels)\n",
    "        feature_loss = F.mse_loss(features, features.detach())  # Dummy loss for feature extraction\n",
    "        topo_loss = F.mse_loss(topo_features, topo_features.detach())  # Dummy loss for topological analysis\n",
    "        \n",
    "        # Scale losses by accumulation steps\n",
    "        feature_loss = feature_loss / accumulation_steps\n",
    "        manifold_loss = manifold_loss / accumulation_steps\n",
    "        topo_loss = topo_loss / accumulation_steps\n",
    "        classifier_loss = classifier_loss / accumulation_steps\n",
    "        \n",
    "        # Backward pass - with retain_graph\n",
    "        feature_loss.backward(retain_graph=True)\n",
    "        manifold_loss.backward(retain_graph=True)\n",
    "        topo_loss.backward(retain_graph=True)\n",
    "        classifier_loss.backward()\n",
    "        \n",
    "        # Update metrics\n",
    "        total_feature_loss += feature_loss.item() * accumulation_steps\n",
    "        total_manifold_loss += manifold_loss.item() * accumulation_steps\n",
    "        total_topo_loss += topo_loss.item() * accumulation_steps\n",
    "        total_classifier_loss += classifier_loss.item() * accumulation_steps\n",
    "        total_loss += (feature_loss.item() + manifold_loss.item() + \n",
    "                      topo_loss.item() + classifier_loss.item()) * accumulation_steps\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += pred.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Only step optimizers after accumulation_steps\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            feature_optimizer.step()\n",
    "            manifold_optimizer.step()\n",
    "            topo_optimizer.step()\n",
    "            classifier_optimizer.step()\n",
    "            \n",
    "            # Zero gradients\n",
    "            feature_optimizer.zero_grad()\n",
    "            manifold_optimizer.zero_grad()\n",
    "            topo_optimizer.zero_grad()\n",
    "            classifier_optimizer.zero_grad()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': total_loss / (batch_idx + 1),\n",
    "            'acc': 100. * correct / total\n",
    "        })\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_feature_loss = total_feature_loss / len(train_loader)\n",
    "    avg_manifold_loss = total_manifold_loss / len(train_loader)\n",
    "    avg_topo_loss = total_topo_loss / len(train_loader)\n",
    "    avg_classifier_loss = total_classifier_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'feature_loss': avg_feature_loss,\n",
    "        'manifold_loss': avg_manifold_loss,\n",
    "        'topo_loss': avg_topo_loss,\n",
    "        'classifier_loss': avg_classifier_loss,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for the validate() function to properly use the dimension adapter\n",
    "def validate(val_loader):\n",
    "    # Set models to evaluation mode\n",
    "    feature_network.eval()\n",
    "    tensor_computer.eval()\n",
    "    dim_adapter.eval()  # Add this line to set the adapter to evaluation mode\n",
    "    manifold_module.eval()\n",
    "    topo_module.eval()\n",
    "    classifier.eval()\n",
    "    \n",
    "    # Initialize metrics\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Store predictions and true labels for confusion matrix\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_uncertainty = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            features, _ = feature_network(images)\n",
    "            phase_tensor = tensor_computer(features)\n",
    "            \n",
    "            # Apply dimension adapter to match channel count - ADD THIS LINE\n",
    "            adapted_tensor = dim_adapter(phase_tensor)\n",
    "            \n",
    "            # Use the adapted tensor instead\n",
    "            manifold_features, _ = manifold_module(adapted_tensor)\n",
    "            point_cloud = point_cloud_generator(manifold_features)\n",
    "            \n",
    "            # Forward pass through topological analysis\n",
    "            topo_features, _ = topo_module(point_cloud)\n",
    "            \n",
    "            # Forward pass through classifier\n",
    "            logits, probs, uncertainty = classifier(manifold_features, topo_features)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = classification_criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += pred.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Store for metrics\n",
    "            all_preds.append(pred.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "            all_uncertainty.append(uncertainty.cpu())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    # Concatenate predictions and labels\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_probs = torch.cat(all_probs)\n",
    "    all_uncertainty = torch.cat(all_uncertainty)\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'probabilities': all_probs,\n",
    "        'uncertainty': all_uncertainty\n",
    "    }\n",
    "\n",
    "# Additional fix for training loop: Don't forget to save the dimension adapter\n",
    "# Add this to your training loop when saving the model\n",
    "def save_model_with_adapter():\n",
    "    torch.save(feature_network.state_dict(), 'models/tiny_feature_network.pth')\n",
    "    torch.save(tensor_computer.state_dict(), 'models/tiny_tensor_computer.pth')\n",
    "    torch.save(dim_adapter.state_dict(), 'models/dim_adapter.pth')  # Save the adapter\n",
    "    torch.save(manifold_module.state_dict(), 'models/manifold_module.pth')\n",
    "    torch.save(topo_module.state_dict(), 'models/tiny_topo_module.pth')\n",
    "    torch.save(classifier.state_dict(), 'models/classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 10\n",
    "gradient_accumulation_steps = 2\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# Metrics tracking\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Train for one epoch using gradient accumulation\n",
    "    train_metrics = train_epoch_with_accumulation(train_loader, epoch, accumulation_steps=gradient_accumulation_steps)\n",
    "    train_losses.append(train_metrics['loss'])\n",
    "    train_accuracies.append(train_metrics['accuracy'])\n",
    "    \n",
    "    # Validate\n",
    "    val_metrics = validate(val_loader)\n",
    "    val_losses.append(val_metrics['loss'])\n",
    "    val_accuracies.append(val_metrics['accuracy'])\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Epoch {epoch}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {train_metrics['loss']:.4f}, Train Acc: {train_metrics['accuracy']:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.2f}%\")\n",
    "    \n",
    "    # Update learning rate schedulers\n",
    "    feature_scheduler.step(val_metrics['loss'])\n",
    "    manifold_scheduler.step(val_metrics['loss'])\n",
    "    topo_scheduler.step(val_metrics['loss'])\n",
    "    classifier_scheduler.step(val_metrics['loss'])\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['accuracy'] > best_val_accuracy:\n",
    "        best_val_accuracy = val_metrics['accuracy']\n",
    "        print(f\"  New best model with accuracy: {best_val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Save each component separately\n",
    "        torch.save(feature_network.state_dict(), 'models/tiny_feature_network.pth')\n",
    "        torch.save(tensor_computer.state_dict(), 'models/tiny_tensor_computer.pth')\n",
    "        torch.save(manifold_module.state_dict(), 'models/manifold_module.pth')\n",
    "        torch.save(topo_module.state_dict(), 'models/tiny_topo_module.pth')\n",
    "        torch.save(classifier.state_dict(), 'models/classifier.pth')\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(f\"Best validation accuracy: {best_val_accuracy:.2f}%\")\n",
    "print(f\"Model saved to 'models/' directory with 'tiny_' prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goatvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
